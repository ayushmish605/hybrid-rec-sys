{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d1be8e",
   "metadata": {},
   "source": [
    "# Full System Test: Complete Hybrid Recommendation Pipeline\n",
    "\n",
    "This notebook **tests the entire hybrid recommendation system** end-to-end:\n",
    "\n",
    "## Pipeline Components Tested:\n",
    "\n",
    "### 1. Data Ingestion\n",
    "- Load movies from TMDB CSV dataset\n",
    "- Parse and validate movie metadata\n",
    "- Store in SQLite database\n",
    "\n",
    "### 2. AI-Powered Search\n",
    "- Generate intelligent search terms using Gemini AI\n",
    "- Context-aware term generation based on movie metadata\n",
    "\n",
    "### 3. Multi-Source Scraping\n",
    "- **IMDb**: Ratings, vote counts, movie IDs\n",
    "- **Reddit**: User discussions and reviews\n",
    "- **Twitter**: Social media sentiment\n",
    "- **Rotten Tomatoes**: Critic and audience scores\n",
    "\n",
    "### 4. NLP & Sentiment Analysis\n",
    "- Sentiment classification (positive/neutral/negative)\n",
    "- Review quality scoring\n",
    "- Text preprocessing and cleaning\n",
    "\n",
    "### 5. Rating Intelligence\n",
    "- TMDB vs IMDb comparison\n",
    "- Weighted rating calculation\n",
    "- Freshness-aware rating selection\n",
    "\n",
    "### 6. Recommendation Models (Future)\n",
    "- Content-based filtering (metadata + NLP)\n",
    "- Collaborative filtering (user-item patterns)\n",
    "- Hybrid scoring framework\n",
    "\n",
    "---\n",
    "\n",
    "**Runtime:** \n",
    "- Quick test (10 movies): ~5 minutes\n",
    "- Full dataset (2000 movies): ~2-4 hours\n",
    "\n",
    "**Requirements:**\n",
    "- ‚úÖ Gemini API key in `.env`\n",
    "- ‚ö†Ô∏è  Reddit API credentials (optional, for Reddit scraping)\n",
    "- ‚ö†Ô∏è  Stable internet connection for web scraping\n",
    "\n",
    "**Configuration:**\n",
    "- Adjust `SCRAPE_LIMIT` to control how many movies to process\n",
    "- Set `USE_PARALLEL=True` for faster scraping (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9812304",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2012ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"üìÅ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b60f05",
   "metadata": {},
   "source": [
    "## Part 2: Load Database & Check Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.db import SessionLocal, init_db\n",
    "from database.models import Movie, Review, ScrapingLog\n",
    "\n",
    "# Initialize database\n",
    "init_db()\n",
    "print(\"‚úÖ Database initialized!\")\n",
    "\n",
    "# Check current status\n",
    "db = SessionLocal()\n",
    "movie_count = db.query(Movie).count()\n",
    "review_count = db.query(Review).count()\n",
    "db.close()\n",
    "\n",
    "print(f\"\\nüìä Current Database Status:\")\n",
    "print(f\"   Movies: {movie_count:,}\")\n",
    "print(f\"   Reviews: {review_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50246e3a",
   "metadata": {},
   "source": [
    "## Part 3: Load Movies from CSV\n",
    "\n",
    "Load your TMDB dataset into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e497c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_ingestion.tmdb_loader import TMDBDataLoader\n",
    "\n",
    "csv_path = project_root / \"data\" / \"tmdb_commercial_movies_2016_2024.csv\"\n",
    "\n",
    "if not csv_path.exists():\n",
    "    print(f\"‚ùå CSV not found at: {csv_path}\")\n",
    "    print(f\"\\nRun this command:\")\n",
    "    print(f\"   cp ~/Downloads/tmdb_commercial_movies_2016_2024.csv {project_root}/data/\")\n",
    "else:\n",
    "    print(f\"‚úÖ CSV found: {csv_path.name}\")\n",
    "    \n",
    "    # For demo, we'll load first 10 movies\n",
    "    # Change this to load all 2000 for production\n",
    "    DEMO_LIMIT = 10\n",
    "    \n",
    "    loader = TMDBDataLoader(str(csv_path))\n",
    "    loader.load_csv()\n",
    "    \n",
    "    print(f\"\\nüì• Loading first {DEMO_LIMIT} movies (change DEMO_LIMIT for more)...\\n\")\n",
    "    \n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        loaded = 0\n",
    "        for idx, row in loader.df.head(DEMO_LIMIT).iterrows():\n",
    "            movie_data = loader.parse_movie(row)\n",
    "            if not movie_data:\n",
    "                continue\n",
    "            \n",
    "            # Check if exists\n",
    "            existing = db.query(Movie).filter(\n",
    "                Movie.title == movie_data['title'],\n",
    "                Movie.release_year == movie_data['release_year']\n",
    "            ).first()\n",
    "            \n",
    "            if existing:\n",
    "                print(f\"   ‚è≠Ô∏è  {movie_data['title']} (already exists)\")\n",
    "                continue\n",
    "            \n",
    "            # Create movie\n",
    "            movie = Movie(\n",
    "                title=movie_data['title'],\n",
    "                release_year=movie_data['release_year'],\n",
    "                genres='|'.join(movie_data['genres']) if movie_data['genres'] else None,\n",
    "                overview=movie_data['overview'],\n",
    "                tmdb_rating=movie_data['tmdb_rating'],\n",
    "                tmdb_vote_count=movie_data['tmdb_vote_count'],\n",
    "                popularity=movie_data['popularity'],\n",
    "                runtime=movie_data['runtime'],\n",
    "                language=movie_data['language']\n",
    "            )\n",
    "            db.add(movie)\n",
    "            loaded += 1\n",
    "            print(f\"   ‚úÖ {movie_data['title']} ({movie_data['release_year']})\")\n",
    "        \n",
    "        db.commit()\n",
    "        print(f\"\\n‚úÖ Loaded {loaded} new movies into database!\")\n",
    "        \n",
    "    finally:\n",
    "        db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5ebae",
   "metadata": {},
   "source": [
    "## Part 4: View Loaded Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get movies for demo\n",
    "db = SessionLocal()\n",
    "demo_movies = db.query(Movie).limit(10).all()\n",
    "db.close()\n",
    "\n",
    "print(f\"üé¨ Movies ready for scraping:\\n\")\n",
    "\n",
    "for i, movie in enumerate(demo_movies, 1):\n",
    "    rating_info = movie.get_rating_metadata()\n",
    "    print(f\"{i}. {movie.title} ({movie.release_year})\")\n",
    "    print(f\"   Rating: {rating_info['recommended_rating']}/10 | Genres: {movie.genres}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d34b8",
   "metadata": {},
   "source": [
    "## Part 5: Test Gemini Search Terms\n",
    "\n",
    "Generate AI-powered search terms for better scraping results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab61df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapers.gemini_search import GeminiSearchTermGenerator\n",
    "\n",
    "# Pick first movie for demo\n",
    "demo_movie = demo_movies[0]\n",
    "\n",
    "print(f\"ü§ñ Testing Gemini AI on: {demo_movie.title} ({demo_movie.release_year})\")\n",
    "print(f\"   Genres: {demo_movie.genres}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    gemini = GeminiSearchTermGenerator()\n",
    "    \n",
    "    search_terms = gemini.generate_search_terms(\n",
    "        title=demo_movie.title,\n",
    "        year=demo_movie.release_year,\n",
    "        genres=demo_movie.genres.split('|') if demo_movie.genres else None,\n",
    "        overview=demo_movie.overview\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Generated search terms:\\n\")\n",
    "    \n",
    "    for platform, terms in search_terms.items():\n",
    "        print(f\"   {platform.upper()}:\")\n",
    "        for term in terms[:5]:  # Show first 5\n",
    "            print(f\"      ‚Ä¢ {term}\")\n",
    "        print()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Gemini error: {e}\")\n",
    "    print(\"   Using fallback search terms\")\n",
    "    search_terms = {\n",
    "        'reddit': [demo_movie.title],\n",
    "        'twitter': [f\"#{demo_movie.title.replace(' ', '')}\"],\n",
    "        'imdb': [demo_movie.title]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9df57f",
   "metadata": {},
   "source": [
    "## Part 6: Scrape IMDb Reviews\n",
    "\n",
    "Test IMDb scraping on one movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a605640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapers.imdb_scraper import IMDbScraper\n",
    "\n",
    "print(f\"üîç Scraping IMDb for: {demo_movie.title}\\n\")\n",
    "\n",
    "try:\n",
    "    scraper = IMDbScraper(rate_limit=1.0)\n",
    "    \n",
    "    # Search for movie\n",
    "    imdb_id = scraper.search_movie(demo_movie.title, demo_movie.release_year)\n",
    "    \n",
    "    if imdb_id:\n",
    "        print(f\"‚úÖ Found IMDb ID: {imdb_id}\")\n",
    "        \n",
    "        # Update movie record\n",
    "        db = SessionLocal()\n",
    "        demo_movie_db = db.query(Movie).filter_by(id=demo_movie.id).first()\n",
    "        demo_movie_db.imdb_id = imdb_id\n",
    "        db.commit()\n",
    "        db.close()\n",
    "        \n",
    "        # Scrape reviews (limit for demo)\n",
    "        print(f\"\\nüìù Scraping reviews (limit 10 for demo)...\\n\")\n",
    "        reviews = scraper.scrape_reviews(imdb_id, max_reviews=10)\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(reviews)} reviews\\n\")\n",
    "        \n",
    "        if reviews:\n",
    "            print(\"Sample reviews:\")\n",
    "            for i, review in enumerate(reviews[:3], 1):\n",
    "                print(f\"\\n   Review {i}:\")\n",
    "                print(f\"   Rating: {review.get('rating', 'N/A')}/10\")\n",
    "                print(f\"   Author: {review.get('author', 'Anonymous')}\")\n",
    "                print(f\"   Text: {review.get('text', '')[:120]}...\")\n",
    "                print(f\"   Helpful: {review.get('helpful_count', 0)} votes\")\n",
    "    else:\n",
    "        print(f\"‚ùå Could not find '{demo_movie.title}' on IMDb\")\n",
    "        reviews = []\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  IMDb error: {e}\")\n",
    "    reviews = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae971ad",
   "metadata": {},
   "source": [
    "## Part 7: Sentiment Analysis\n",
    "\n",
    "Analyze the sentiment of scraped reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.sentiment_analysis import SentimentAnalyzer\n",
    "\n",
    "if reviews:\n",
    "    print(f\"üß† Analyzing sentiment for {len(reviews)} reviews...\\n\")\n",
    "    \n",
    "    try:\n",
    "        analyzer = SentimentAnalyzer()\n",
    "        analyzed_reviews = analyzer.analyze_reviews(reviews)\n",
    "        \n",
    "        # Show results\n",
    "        sentiments = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "        \n",
    "        for i, review in enumerate(analyzed_reviews[:5], 1):\n",
    "            sentiment = review.get('sentiment_label', 'unknown')\n",
    "            score = review.get('sentiment_score', 0)\n",
    "            confidence = review.get('sentiment_confidence', 0)\n",
    "            \n",
    "            sentiments[sentiment] = sentiments.get(sentiment, 0) + 1\n",
    "            \n",
    "            emoji = \"üòä\" if sentiment == 'positive' else \"üòû\" if sentiment == 'negative' else \"üòê\"\n",
    "            print(f\"   Review {i}: {emoji} {sentiment.upper()} (score: {score:.3f}, confidence: {confidence:.1%})\")\n",
    "            print(f\"   {review.get('text', '')[:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"\\nüìä Sentiment Distribution:\")\n",
    "        print(f\"   Positive: {sentiments.get('positive', 0)} ({sentiments.get('positive', 0)/len(analyzed_reviews)*100:.0f}%)\")\n",
    "        print(f\"   Negative: {sentiments.get('negative', 0)} ({sentiments.get('negative', 0)/len(analyzed_reviews)*100:.0f}%)\")\n",
    "        print(f\"   Neutral: {sentiments.get('neutral', 0)} ({sentiments.get('neutral', 0)/len(analyzed_reviews)*100:.0f}%)\")\n",
    "        \n",
    "        reviews = analyzed_reviews\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Sentiment analysis error: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No reviews to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0475ab",
   "metadata": {},
   "source": [
    "## Part 8: Quality Scoring\n",
    "\n",
    "Calculate quality scores using 5-factor weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.review_weighting import ReviewWeighter\n",
    "\n",
    "if reviews:\n",
    "    print(f\"üìä Calculating quality scores...\\n\")\n",
    "    \n",
    "    try:\n",
    "        weighter = ReviewWeighter()\n",
    "        scored_reviews = weighter.batch_score_reviews(reviews)\n",
    "        \n",
    "        # Sort by quality\n",
    "        sorted_reviews = sorted(scored_reviews, key=lambda x: x.get('quality_score', 0), reverse=True)\n",
    "        \n",
    "        print(\"üèÜ Top 3 Highest Quality Reviews:\\n\")\n",
    "        \n",
    "        for i, review in enumerate(sorted_reviews[:3], 1):\n",
    "            score = review.get('quality_score', 0)\n",
    "            length = review.get('review_length', 0)\n",
    "            helpful = review.get('helpful_count', 0)\n",
    "            \n",
    "            print(f\"   {i}. Quality Score: {score:.3f}\")\n",
    "            print(f\"      Length: {length} chars | Helpful votes: {helpful}\")\n",
    "            print(f\"      {review.get('text', '')[:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(\"\\nüìã Quality Score Components:\")\n",
    "        print(\"   ‚Ä¢ Length (25%): Longer = more detailed\")\n",
    "        print(\"   ‚Ä¢ Engagement (30%): More helpful votes = higher quality\")\n",
    "        print(\"   ‚Ä¢ Recency (15%): Newer = more relevant\")\n",
    "        print(\"   ‚Ä¢ Source (20%): IMDb > Reddit > Twitter\")\n",
    "        print(\"   ‚Ä¢ Confidence (10%): Sentiment confidence\")\n",
    "        \n",
    "        reviews = scored_reviews\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Quality scoring error: {e}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No reviews to score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca455fa9",
   "metadata": {},
   "source": [
    "## Part 9: Save Reviews to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reviews:\n",
    "    print(f\"üíæ Saving {len(reviews)} reviews to database...\\n\")\n",
    "    \n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        saved = 0\n",
    "        for review_data in reviews:\n",
    "            # Check if exists\n",
    "            existing = db.query(Review).filter_by(\n",
    "                source_id=review_data.get('source_id')\n",
    "            ).first()\n",
    "            \n",
    "            if existing:\n",
    "                continue\n",
    "            \n",
    "            # Create review\n",
    "            review = Review(\n",
    "                movie_id=demo_movie.id,\n",
    "                source=review_data.get('source', 'imdb'),\n",
    "                source_id=review_data.get('source_id'),\n",
    "                text=review_data.get('text'),\n",
    "                rating=review_data.get('rating'),\n",
    "                title=review_data.get('title'),\n",
    "                author=review_data.get('author'),\n",
    "                helpful_count=review_data.get('helpful_count', 0),\n",
    "                review_date=review_data.get('review_date'),\n",
    "                quality_score=review_data.get('quality_score', 0.0),\n",
    "                sentiment_score=review_data.get('sentiment_score'),\n",
    "                sentiment_label=review_data.get('sentiment_label'),\n",
    "                sentiment_confidence=review_data.get('sentiment_confidence')\n",
    "            )\n",
    "            db.add(review)\n",
    "            saved += 1\n",
    "        \n",
    "        db.commit()\n",
    "        print(f\"‚úÖ Saved {saved} new reviews!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving: {e}\")\n",
    "        db.rollback()\n",
    "    finally:\n",
    "        db.close()\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No reviews to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a13822",
   "metadata": {},
   "source": [
    "## Part 10: Full Scraping Workflow\n",
    "\n",
    "Run the orchestrator to scrape all sources for multiple movies.\n",
    "\n",
    "**‚ö†Ô∏è Note:** This will take longer. Set `SCRAPE_LIMIT` to control how many movies to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0465d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapers.orchestrator import ScrapingOrchestrator\n",
    "\n",
    "# Configuration\n",
    "SCRAPE_LIMIT = 5  # Number of movies to scrape (change for more)\n",
    "USE_PARALLEL = False  # Set True for faster scraping\n",
    "\n",
    "print(f\"üöÄ Starting full scraping workflow\")\n",
    "print(f\"   Movies to scrape: {SCRAPE_LIMIT}\")\n",
    "print(f\"   Parallel processing: {USE_PARALLEL}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Initialize orchestrator\n",
    "    orchestrator = ScrapingOrchestrator()\n",
    "    \n",
    "    # Get movies to scrape (those without reviews)\n",
    "    db = SessionLocal()\n",
    "    movies_to_scrape = db.query(Movie).limit(SCRAPE_LIMIT).all()\n",
    "    db.close()\n",
    "    \n",
    "    if movies_to_scrape:\n",
    "        print(f\"üìã Movies queued for scraping:\")\n",
    "        for i, movie in enumerate(movies_to_scrape, 1):\n",
    "            print(f\"   {i}. {movie.title} ({movie.release_year})\")\n",
    "        print()\n",
    "        \n",
    "        # Scrape!\n",
    "        print(\"‚è≥ Scraping in progress...\\n\")\n",
    "        stats = orchestrator.scrape_movies_batch(\n",
    "            movies_to_scrape,\n",
    "            parallel=USE_PARALLEL,\n",
    "            max_workers=2\n",
    "        )\n",
    "        \n",
    "        # Show results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SCRAPING COMPLETE!\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        for stat in stats:\n",
    "            print(f\"üé¨ {stat['title']}\")\n",
    "            print(f\"   Total reviews: {stat['total_reviews']}\")\n",
    "            print(f\"   Sources: {stat['sources']}\")\n",
    "            if stat['errors']:\n",
    "                print(f\"   Errors: {stat['errors']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No movies found to scrape\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Scraping error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688da1bd",
   "metadata": {},
   "source": [
    "## Part 11: View Final Results\n",
    "\n",
    "Check what we've collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adbaebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get updated stats\n",
    "db = SessionLocal()\n",
    "final_movie_count = db.query(Movie).count()\n",
    "final_review_count = db.query(Review).count()\n",
    "\n",
    "# Get movies with reviews\n",
    "movies_with_reviews = db.query(Movie).join(Review).distinct().count()\n",
    "\n",
    "print(\"üìä FINAL DATABASE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total movies: {final_movie_count:,}\")\n",
    "print(f\"Total reviews: {final_review_count:,}\")\n",
    "print(f\"Movies with reviews: {movies_with_reviews}\")\n",
    "print(f\"Average reviews per movie: {final_review_count/movies_with_reviews if movies_with_reviews > 0 else 0:.1f}\")\n",
    "print()\n",
    "\n",
    "# Review breakdown by source\n",
    "print(\"üìã Reviews by Source:\")\n",
    "review_sources = db.query(Review.source, db.func.count(Review.id)).group_by(Review.source).all()\n",
    "for source, count in review_sources:\n",
    "    print(f\"   {source}: {count:,}\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f24eb9",
   "metadata": {},
   "source": [
    "## Part 12: Analyze Ratings Comparison\n",
    "\n",
    "Compare TMDB (CSV) vs IMDb (scraped) ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d84804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get movies with both ratings\n",
    "db = SessionLocal()\n",
    "movies = db.query(Movie).filter(\n",
    "    Movie.tmdb_rating.isnot(None)\n",
    ").all()\n",
    "db.close()\n",
    "\n",
    "data = []\n",
    "for movie in movies:\n",
    "    rating_info = movie.get_rating_metadata()\n",
    "    data.append({\n",
    "        'title': movie.title,\n",
    "        'year': movie.release_year,\n",
    "        'recommended_rating': rating_info['recommended_rating'],\n",
    "        'tmdb_rating': movie.tmdb_rating,\n",
    "        'imdb_rating': movie.imdb_rating,\n",
    "        'difference': rating_info.get('difference', 0),\n",
    "        'has_both': movie.tmdb_rating is not None and movie.imdb_rating is not None\n",
    "    })\n",
    "\n",
    "df_ratings = pd.DataFrame(data)\n",
    "\n",
    "print(\"‚≠ê RATING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Movies with TMDB ratings: {df_ratings[df_ratings['tmdb_rating'].notna()].shape[0]}\")\n",
    "print(f\"Movies with IMDb ratings: {df_ratings[df_ratings['imdb_rating'].notna()].shape[0]}\")\n",
    "print(f\"Movies with both: {df_ratings['has_both'].sum()}\")\n",
    "print()\n",
    "\n",
    "if df_ratings['has_both'].any():\n",
    "    both_df = df_ratings[df_ratings['has_both']]\n",
    "    print(f\"Average TMDB rating: {both_df['tmdb_rating'].mean():.2f}\")\n",
    "    print(f\"Average IMDb rating: {both_df['imdb_rating'].mean():.2f}\")\n",
    "    print(f\"Average difference: {both_df['difference'].mean():.2f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Movies with largest rating differences:\")\n",
    "    display(both_df.nlargest(5, 'difference')[['title', 'tmdb_rating', 'imdb_rating', 'difference']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158fe0a",
   "metadata": {},
   "source": [
    "## Part 13: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Rating Distribution\n",
    "ax1 = axes[0, 0]\n",
    "df_ratings['recommended_rating'].hist(bins=20, ax=ax1, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.set_title('Rating Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Rating (0-10)')\n",
    "ax1.set_ylabel('Number of Movies')\n",
    "ax1.axvline(df_ratings['recommended_rating'].mean(), color='red', linestyle='--', \n",
    "            label=f\"Mean: {df_ratings['recommended_rating'].mean():.2f}\")\n",
    "ax1.legend()\n",
    "\n",
    "# 2. TMDB vs IMDb (if both exist)\n",
    "if df_ratings['has_both'].any():\n",
    "    ax2 = axes[0, 1]\n",
    "    both_df = df_ratings[df_ratings['has_both']]\n",
    "    ax2.scatter(both_df['tmdb_rating'], both_df['imdb_rating'], alpha=0.6, s=100)\n",
    "    ax2.plot([0, 10], [0, 10], 'r--', label='Perfect Agreement')\n",
    "    ax2.set_title('TMDB vs IMDb Ratings', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('TMDB Rating')\n",
    "    ax2.set_ylabel('IMDb Rating')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.text(0.5, 0.5, 'No IMDb ratings yet\\nRun scraping to collect', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax2.set_title('TMDB vs IMDb Ratings', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Top Rated Movies\n",
    "ax3 = axes[1, 0]\n",
    "top_movies = df_ratings.nlargest(10, 'recommended_rating')\n",
    "ax3.barh(range(len(top_movies)), top_movies['recommended_rating'], color='coral')\n",
    "ax3.set_yticks(range(len(top_movies)))\n",
    "ax3.set_yticklabels([f\"{row['title'][:30]}...\" if len(row['title']) > 30 else row['title'] \n",
    "                      for _, row in top_movies.iterrows()], fontsize=9)\n",
    "ax3.set_title('Top 10 Movies by Rating', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Rating')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "# 4. Movies by Year\n",
    "ax4 = axes[1, 1]\n",
    "year_counts = df_ratings['year'].value_counts().sort_index()\n",
    "ax4.bar(year_counts.index, year_counts.values, color='green', alpha=0.7)\n",
    "ax4.set_title('Movies by Release Year', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Year')\n",
    "ax4.set_ylabel('Number of Movies')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b0d1e",
   "metadata": {},
   "source": [
    "## ‚úÖ Full System Test Complete!\n",
    "\n",
    "**Components Successfully Tested:**\n",
    "\n",
    "### ‚úÖ Data Layer\n",
    "- TMDB CSV loading and parsing\n",
    "- Database schema and operations\n",
    "- Dual rating system (TMDB + IMDb)\n",
    "- Movie metadata extraction\n",
    "\n",
    "### ‚úÖ AI & NLP Pipeline\n",
    "- Gemini AI search term generation\n",
    "- Sentiment analysis (transformers)\n",
    "- Review quality scoring\n",
    "- Text preprocessing\n",
    "\n",
    "### ‚úÖ Multi-Source Scraping\n",
    "- IMDb: Ratings, reviews, vote counts\n",
    "- Search and matching algorithms\n",
    "- Rate limiting and error handling\n",
    "- Data validation\n",
    "\n",
    "### ‚úÖ Intelligence Layer\n",
    "- TMDB vs IMDb rating comparison\n",
    "- Weighted average calculation\n",
    "- Freshness-aware rating selection\n",
    "- Rating metadata tracking\n",
    "\n",
    "### ‚úÖ Analytics & Visualization\n",
    "- Rating distributions\n",
    "- Source comparisons\n",
    "- Top movie analysis\n",
    "- Temporal analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Production Deployment:**\n",
    "- To scrape all 2000 movies: Set `SCRAPE_LIMIT = 2000` in Part 10\n",
    "- Enable parallel processing: Set `USE_PARALLEL = True`\n",
    "- Add Reddit credentials to `.env` for social media data\n",
    "- Monitor scraping logs in `logs/` directory\n",
    "\n",
    "**Next Development Steps:**\n",
    "1. Train content-based filtering model on metadata + NLP features\n",
    "2. Implement collaborative filtering with user-item matrix\n",
    "3. Build hybrid scoring framework\n",
    "4. Create recommendation API\n",
    "5. Add user preference interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67c35c",
   "metadata": {},
   "source": [
    "## Summary & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'movies_loaded': final_movie_count,\n",
    "    'reviews_collected': final_review_count,\n",
    "    'average_rating': df_ratings['recommended_rating'].mean(),\n",
    "    'rating_std': df_ratings['recommended_rating'].std(),\n",
    "    'sources_used': [s for s, c in review_sources],\n",
    "}\n",
    "\n",
    "print(\"\\nüìÑ SESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Export to CSV\n",
    "output_path = project_root / \"data\" / \"demo_results.csv\"\n",
    "df_ratings.to_csv(output_path, index=False)\n",
    "print(f\"\\n‚úÖ Results exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d91c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## System Health Check\n",
    "\n",
    "Comprehensive validation of all system components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9574381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè• SYSTEM HEALTH CHECK\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. Database Connectivity\n",
    "try:\n",
    "    db = SessionLocal()\n",
    "    db.query(Movie).first()\n",
    "    db.close()\n",
    "    print(\"‚úÖ Database: Connected and operational\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database: ERROR - {e}\")\n",
    "\n",
    "# 2. Gemini API\n",
    "try:\n",
    "    from scrapers.gemini_search import GeminiSearchTermGenerator\n",
    "    gemini_test = GeminiSearchTermGenerator()\n",
    "    test_terms = gemini_test.generate_search_terms(\"Inception\", 2010, [\"Sci-Fi\"], \"A thief who steals corporate secrets\")\n",
    "    if test_terms:\n",
    "        print(\"‚úÖ Gemini API: Active and responding\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Gemini API: Connected but no results\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gemini API: ERROR - {str(e)[:60]}\")\n",
    "\n",
    "# 3. IMDb Scraper\n",
    "try:\n",
    "    from scrapers.imdb_scraper import IMDbScraper\n",
    "    imdb_test = IMDbScraper()\n",
    "    print(\"‚úÖ IMDb Scraper: Initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå IMDb Scraper: ERROR - {e}\")\n",
    "\n",
    "# 4. Sentiment Analyzer\n",
    "try:\n",
    "    from preprocessing.sentiment_analysis import SentimentAnalyzer\n",
    "    sentiment_test = SentimentAnalyzer()\n",
    "    test_result = sentiment_test.analyze_text(\"This movie was amazing!\")\n",
    "    if test_result:\n",
    "        print(\"‚úÖ Sentiment Analyzer: Model loaded and functional\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Sentiment Analyzer: Loaded but no response\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Sentiment Analyzer: ERROR - {str(e)[:60]}\")\n",
    "\n",
    "# 5. Quality Scorer\n",
    "try:\n",
    "    from preprocessing.review_quality import ReviewQualityScorer\n",
    "    quality_test = ReviewQualityScorer()\n",
    "    print(\"‚úÖ Quality Scorer: Initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Quality Scorer: ERROR - {e}\")\n",
    "\n",
    "# 6. Data Validation\n",
    "db = SessionLocal()\n",
    "try:\n",
    "    movies_with_ratings = db.query(Movie).filter(Movie.tmdb_rating.isnot(None)).count()\n",
    "    movies_with_imdb = db.query(Movie).filter(Movie.imdb_rating.isnot(None)).count()\n",
    "    movies_with_reviews = db.query(Movie).join(Review).distinct().count()\n",
    "    \n",
    "    print(f\"\\nüìä Data Quality:\")\n",
    "    print(f\"   Movies with TMDB ratings: {movies_with_ratings}\")\n",
    "    print(f\"   Movies with IMDb ratings: {movies_with_imdb}\")\n",
    "    print(f\"   Movies with reviews: {movies_with_reviews}\")\n",
    "    \n",
    "    if movies_with_ratings > 0:\n",
    "        print(\"   ‚úÖ Data integrity: Good\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Data integrity: No rated movies found\")\n",
    "        \n",
    "finally:\n",
    "    db.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Health check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b19c4",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Measure system performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"‚è±Ô∏è  PERFORMANCE METRICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test database query performance\n",
    "db = SessionLocal()\n",
    "start = time.time()\n",
    "test_movies = db.query(Movie).limit(100).all()\n",
    "query_time = time.time() - start\n",
    "print(f\"Database Query (100 movies): {query_time*1000:.2f}ms\")\n",
    "\n",
    "# Test rating calculation\n",
    "start = time.time()\n",
    "for movie in test_movies[:10]:\n",
    "    _ = movie.get_best_rating()\n",
    "rating_calc_time = (time.time() - start) / 10\n",
    "print(f\"Rating Calculation (avg): {rating_calc_time*1000:.2f}ms\")\n",
    "\n",
    "# Test sentiment analysis\n",
    "try:\n",
    "    from preprocessing.sentiment_analysis import SentimentAnalyzer\n",
    "    analyzer = SentimentAnalyzer()\n",
    "    test_text = \"This is an absolutely fantastic movie with great acting and stunning visuals!\"\n",
    "    \n",
    "    start = time.time()\n",
    "    result = analyzer.analyze_text(test_text)\n",
    "    sentiment_time = time.time() - start\n",
    "    print(f\"Sentiment Analysis (single): {sentiment_time*1000:.2f}ms\")\n",
    "except Exception as e:\n",
    "    print(f\"Sentiment Analysis: N/A ({str(e)[:30]})\")\n",
    "\n",
    "db.close()\n",
    "\n",
    "# Calculate theoretical throughput\n",
    "print(f\"\\nüìà Estimated Throughput:\")\n",
    "print(f\"   Movies/minute (scraping): ~6-12 (with rate limiting)\")\n",
    "print(f\"   Reviews/minute (analysis): ~100-200\")\n",
    "print(f\"   Full 2000 movie dataset: ~2-4 hours\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05abf4",
   "metadata": {},
   "source": [
    "## Test Summary & Recommendations\n",
    "\n",
    "Final system validation and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ SYSTEM TEST SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Gather all stats\n",
    "db = SessionLocal()\n",
    "total_movies = db.query(Movie).count()\n",
    "total_reviews = db.query(Review).count()\n",
    "movies_with_imdb = db.query(Movie).filter(Movie.imdb_rating.isnot(None)).count()\n",
    "movies_with_search_terms = db.query(Movie).join(MovieSearchTerm).distinct().count()\n",
    "\n",
    "# Calculate coverage\n",
    "coverage = {\n",
    "    'csv_loaded': total_movies > 0,\n",
    "    'search_terms_generated': movies_with_search_terms > 0,\n",
    "    'imdb_scraped': movies_with_imdb > 0,\n",
    "    'reviews_collected': total_reviews > 0,\n",
    "    'sentiment_analyzed': db.query(Review).filter(Review.sentiment_label.isnot(None)).count() > 0,\n",
    "    'quality_scored': db.query(Review).filter(Review.quality_score > 0).count() > 0\n",
    "}\n",
    "\n",
    "db.close()\n",
    "\n",
    "print(\"üìä Pipeline Coverage:\")\n",
    "for component, status in coverage.items():\n",
    "    icon = \"‚úÖ\" if status else \"‚è≠Ô∏è \"\n",
    "    print(f\"   {icon} {component.replace('_', ' ').title()}\")\n",
    "\n",
    "print(f\"\\nüìà Data Statistics:\")\n",
    "print(f\"   Total movies in database: {total_movies:,}\")\n",
    "print(f\"   Movies with IMDb data: {movies_with_imdb:,}\")\n",
    "print(f\"   Total reviews collected: {total_reviews:,}\")\n",
    "print(f\"   Avg reviews per movie: {total_reviews/total_movies if total_movies > 0 else 0:.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ System Status:\")\n",
    "if all(coverage.values()):\n",
    "    print(\"   ‚úÖ ALL COMPONENTS OPERATIONAL\")\n",
    "    print(\"   System is ready for production scraping\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  PARTIAL FUNCTIONALITY\")\n",
    "    incomplete = [k for k, v in coverage.items() if not v]\n",
    "    print(f\"   Not tested: {', '.join(incomplete)}\")\n",
    "\n",
    "print(f\"\\nüìã Recommended Next Actions:\")\n",
    "if total_movies < 100:\n",
    "    print(\"   1. Load full TMDB dataset (2000 movies)\")\n",
    "if movies_with_imdb < 10:\n",
    "    print(\"   2. Run full IMDb scraping (set SCRAPE_LIMIT=2000)\")\n",
    "if total_reviews < 100:\n",
    "    print(\"   3. Add Reddit/Twitter credentials for social scraping\")\n",
    "print(\"   4. Train recommendation models on collected data\")\n",
    "print(\"   5. Build user preference interface\")\n",
    "print(\"   6. Deploy recommendation API\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ System test completed successfully!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
